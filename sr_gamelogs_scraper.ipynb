{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install scrapy\n",
    "!pip install crochet\n",
    "!pip install cloudscraper\n",
    "!pip install pandas\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "url = \"https://www.sports-reference.com/cbb/players/sienna-betts-1.html\"\n",
    "html = requests.get(url).text\n",
    "print(html)"
   ],
   "id": "b7960613ea0ebb9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "url = \"https://www.sports-reference.com/cbb/schools/\"\n",
    "html = requests.get(url).text\n",
    "# print(html)\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# 1. Find the wrapper\n",
    "wrapper = sel.css('#all_NCAAW_schools')\n",
    "\n",
    "# 2. Try to get the comment inside\n",
    "comment = wrapper.xpath('comment()').get()\n",
    "\n",
    "if comment:\n",
    "    # It was hidden! Clean it.\n",
    "    clean_html = comment.replace('', '')\n",
    "    table_sel = Selector(text=clean_html)\n",
    "    print(\"Found table in comments. Unwrapping...\")\n",
    "else:\n",
    "    # It wasn't hidden! Use the wrapper directly.\n",
    "    table_sel = wrapper\n",
    "    print(\"Table was not commented. Reading directly...\")\n",
    "\n",
    "# 3. Extract the links\n",
    "links_list = [\n",
    "    f\"https://www.sports-reference.com{link}\"\n",
    "    for link in table_sel.css('td[data-stat=\"school_name\"] a::attr(href)').getall()\n",
    "]\n",
    "\n",
    "print(links_list)\n",
    "\n",
    "print(f\"Successfully found {len(links_list)} links.\")\n"
   ],
   "id": "6599cdb54e930c5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "})\n",
    "\n",
    "def get_advanced_stats_df(school_url, year, session, verbose=True):\n",
    "    season_val = f\"{year-1}-{str(year)[-2:]}\"\n",
    "\n",
    "    parts = [p for p in school_url.split('/') if p]\n",
    "    school_slug = parts[-2] if parts[-1] == 'women' else parts[-1]\n",
    "    target_url = f\"{school_url.rstrip('/')}/{year}.html\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[FETCHING] {target_url}\")\n",
    "\n",
    "    try:\n",
    "        resp = session.get(target_url, timeout=10)\n",
    "\n",
    "        if resp.status_code == 429:\n",
    "            print(\"[RATE LIMITED]\")\n",
    "            return \"BLOCKED\"\n",
    "        if resp.status_code != 200:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        html_content = resp.text\n",
    "\n",
    "        # ---- REGEX EXTRACT THE ADVANCED TABLE ----\n",
    "        match = re.search(\n",
    "            r'(<table[^>]*id=\"players_advanced\"[^>]*>.*?</table>)',\n",
    "            html_content,\n",
    "            re.DOTALL\n",
    "        )\n",
    "\n",
    "        if not match:\n",
    "            if verbose:\n",
    "                print(f\"[NO ADV TABLE FOUND] {target_url}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        table_html = match.group(1)\n",
    "\n",
    "        # ---- PARSE CLEAN TABLE HTML ----\n",
    "        sel = Selector(text=table_html)\n",
    "        rows = sel.css('tbody tr')\n",
    "\n",
    "        if not rows:\n",
    "            if verbose:\n",
    "                print(f\"[EMPTY TABLE] {target_url}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        all_rows = []\n",
    "\n",
    "        for row in rows:\n",
    "            if row.css('th[data-stat=\"ranker\"]::text').get() is None:\n",
    "                continue\n",
    "\n",
    "            row_data = {\n",
    "                cell.attrib['data-stat']: cell.css('::text').get()\n",
    "                for cell in row.css('th, td')\n",
    "                if 'data-stat' in cell.attrib\n",
    "            }\n",
    "\n",
    "            player_link = row.css(\n",
    "                'td[data-stat=\"name_display\"] a::attr(href)'\n",
    "            ).get()\n",
    "\n",
    "            row_data['player_sr_link'] = (\n",
    "                f\"https://www.sports-reference.com{player_link}\"\n",
    "                if player_link else None\n",
    "            )\n",
    "            row_data['school'] = school_slug\n",
    "            row_data['season'] = season_val\n",
    "\n",
    "            all_rows.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(all_rows)\n",
    "\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        # ---- CLEANUP ----\n",
    "        df = df.rename(columns={'name_display': 'player_name'})\n",
    "        df = df.drop(columns=[c for c in ['ranker', 'awards'] if c in df.columns])\n",
    "\n",
    "        first_cols = ['player_sr_link', 'player_name', 'school', 'season']\n",
    "        df = df[first_cols + [c for c in df.columns if c not in first_cols]]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[ERROR] {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# get_stats_df(links_list[0], 2025)\n",
    "get_advanced_stats_df(links_list[0], 2025, session)"
   ],
   "id": "fd44225639e57fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:19:10.287198Z",
     "start_time": "2026-02-16T22:19:10.266222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def scrape_multi_year_data(urls, years_list, base_folder=\"data\"):\n",
    "    # Safety catch for single strings\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    # Reverse the years list to go in reverse chronological order\n",
    "    years_to_scrape = sorted(years_list, reverse=True)\n",
    "\n",
    "    for year in years_to_scrape:\n",
    "        # Create year folder (only if not exists)\n",
    "        year_folder = os.path.join(base_folder, str(year))\n",
    "        os.makedirs(year_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"========== STARTING YEAR: {year} ==========\")\n",
    "\n",
    "        for url_idx, url in enumerate(urls):\n",
    "            # Extract school slug\n",
    "            parts = [p for p in url.split('/') if p]\n",
    "            school_slug = parts[-2] if parts[-1] == 'women' else parts[-1]\n",
    "\n",
    "            print(f\"[{year}] Processing {url_idx + 1}/{len(urls)}: {school_slug}\")\n",
    "\n",
    "            # Fetch the data\n",
    "            result = get_advanced_stats_df(url, year, session)\n",
    "\n",
    "            # Block detection\n",
    "            if isinstance(result, str) and result == \"BLOCKED\":\n",
    "                print(f\"!!! BLOCKED !!! Stopped at {school_slug} in {year}. Restart later.\")\n",
    "                return\n",
    "\n",
    "            if not result.empty:\n",
    "                # Path: data/2026/school_slug_wbb_2026.csv\n",
    "                filename = f\"{school_slug}_wbb_advanced_{year}.csv\"\n",
    "                file_path = os.path.join(year_folder, filename)\n",
    "\n",
    "                # to_csv defaults to overwriting the file\n",
    "                result.to_csv(file_path, index=False)\n",
    "                print(f\"      Saved: {file_path}\")\n",
    "            else:\n",
    "                print(f\"      No data found for {year}.\")\n",
    "\n",
    "            # The 4-second \"Polite Scraper\" delay\n",
    "            time.sleep(4)\n",
    "\n",
    "        print(f\"========== COMPLETED YEAR: {year} ==========\\n\")\n",
    "\n",
    "    print(\"Scrape complete.\")"
   ],
   "id": "2fe1b3856210610f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T06:32:27.417583Z",
     "start_time": "2026-02-15T06:05:06.078243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_years = [2024]\n",
    "scrape_multi_year_data(links_list, my_years)"
   ],
   "id": "278c9bcded59cab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== COMPLETED YEAR: 2024 ==========\n",
      "\n",
      "Scrape complete.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:19:18.146108Z",
     "start_time": "2026-02-16T22:19:18.119204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "# Version 1\n",
    "\n",
    "def scrape_gamelogs_to_team_folders(\n",
    "    year,\n",
    "    base_folder=\"data\",\n",
    "    output_folder=\"gamelogs\",\n",
    "    sleep_time=2,\n",
    "    max_retries=5,\n",
    "    verbose=True,\n",
    "    successCount=0,\n",
    "    count=0\n",
    "):\n",
    "    season_folder = os.path.join(base_folder, str(year))\n",
    "\n",
    "    if not os.path.exists(season_folder):\n",
    "        print(f\"[ERROR] Folder not found: {season_folder}\")\n",
    "        return\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "    csv_files = [f for f in os.listdir(season_folder) if f.endswith(\".csv\")]\n",
    "    print(f\"Found {len(csv_files)} school CSVs for {year}\")\n",
    "\n",
    "    for file_idx, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(season_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if \"player_sr_link\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            count += 1\n",
    "            player_url = row.get(\"player_sr_link\")\n",
    "            player_name = row.get(\"player_name\")\n",
    "            school = row.get(\"school\")\n",
    "            season = row.get(\"season\")\n",
    "\n",
    "            if not isinstance(player_url, str):\n",
    "                continue\n",
    "\n",
    "            safe_name = re.sub(r'[^a-zA-Z0-9_]+', '_', str(player_name))\n",
    "            safe_school = re.sub(r'[^a-zA-Z0-9_]+', '_', str(school))\n",
    "\n",
    "            # --- create team folder ---\n",
    "            team_folder = os.path.join(output_folder, str(year), safe_school)\n",
    "            os.makedirs(team_folder, exist_ok=True)\n",
    "\n",
    "            output_file = os.path.join(\n",
    "                team_folder,\n",
    "                f\"{safe_name}_{safe_school}_{year}.csv\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                if verbose:\n",
    "                    print(f\"[SKIP EXISTS] {output_file}\")\n",
    "                continue\n",
    "\n",
    "            player_url = player_url.replace(\".html\", \"\").rstrip(\"/\")\n",
    "            gamelog_url = f\"{player_url}/gamelog/{year}\"\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n[{file_idx+1}/{len(csv_files)} | player {idx+1}] {player_name}\")\n",
    "                print(f\"[URL] {gamelog_url}\")\n",
    "\n",
    "            success = False\n",
    "\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    resp = session.get(gamelog_url, timeout=30)\n",
    "\n",
    "                    if resp.status_code == 429:\n",
    "                        print(\"[RATE LIMITED — sleeping 90s]\")\n",
    "                        time.sleep(90)\n",
    "                        continue\n",
    "\n",
    "                    if resp.status_code != 200:\n",
    "                        break\n",
    "\n",
    "                    html = resp.text\n",
    "                    sel = Selector(text=html)\n",
    "\n",
    "                    table = sel.css(\"table#player_game_log\")\n",
    "\n",
    "                    # --- comment tables ---\n",
    "                    if not table:\n",
    "                        comments = sel.xpath(\"//comment()\").getall()\n",
    "                        for c in comments:\n",
    "                            if \"player_game_log\" in c:\n",
    "                                match = re.search(\n",
    "                                    r'(<table[^>]*id=\"player_game_log\"[^>]*>.*?</table>)',\n",
    "                                    c,\n",
    "                                    re.DOTALL\n",
    "                                )\n",
    "                                if match:\n",
    "                                    table_html = match.group(1)\n",
    "                                    table = Selector(text=table_html)\n",
    "                                    break\n",
    "\n",
    "                    # --- regex fallback ---\n",
    "                    if not table:\n",
    "                        match = re.search(\n",
    "                            r'(<table[^>]*id=\"player_game_log\"[^>]*>.*?</table>)',\n",
    "                            html,\n",
    "                            re.DOTALL\n",
    "                        )\n",
    "                        if match:\n",
    "                            table_html = match.group(1)\n",
    "                            table = Selector(text=table_html)\n",
    "\n",
    "                    if not table:\n",
    "                        print(\"[NO TABLE]\")\n",
    "                        break\n",
    "\n",
    "                    rows = table.css(\"tbody tr\")\n",
    "                    player_games = []\n",
    "\n",
    "                    for r in rows:\n",
    "                        if \"thead\" in r.attrib.get(\"class\", \"\"):\n",
    "                            continue\n",
    "\n",
    "                        game = {\n",
    "                            cell.attrib[\"data-stat\"]: cell.css(\"::text\").get()\n",
    "                            for cell in r.css(\"th, td\")\n",
    "                            if \"data-stat\" in cell.attrib\n",
    "                        }\n",
    "\n",
    "                        game[\"player_name\"] = player_name\n",
    "                        game[\"player_sr_link\"] = player_url + \".html\" # gotta add html back so link works\n",
    "                        game[\"school\"] = school\n",
    "                        game[\"season\"] = season\n",
    "\n",
    "                        player_games.append(game)\n",
    "\n",
    "                    if player_games:\n",
    "                        pd.DataFrame(player_games).to_csv(output_file, index=False)\n",
    "                        print(f\"[SAVED] {output_file}\")\n",
    "                        successCount += 1\n",
    "                        success = True\n",
    "                        break\n",
    "\n",
    "                except requests.exceptions.ReadTimeout:\n",
    "                    print(f\"[TIMEOUT attempt {attempt+1}] retrying…\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {player_name}: {e}\")\n",
    "                    break\n",
    "\n",
    "            if not success:\n",
    "                print(f\"[FAILED] {player_name}\")\n",
    "            print(f\"{successCount}/{count} have been successfully scraped\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(\"\\nDone scraping all players.\")\n"
   ],
   "id": "ea98060527b25c34",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scrape_gamelogs_to_team_folders(year=2024)",
   "id": "c534a8bb2e0d0384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scrape_gamelogs_to_team_folders(year=2023)\n",
    "scrape_gamelogs_to_team_folders(year=2022)\n",
    "scrape_gamelogs_to_team_folders(year=2021)"
   ],
   "id": "b052a1e5b0709d04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:20:05.071265Z",
     "start_time": "2026-02-16T22:20:05.043738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "# Version 2\n",
    "\n",
    "def scrape_gamelogs_to_team_folders_v2(\n",
    "    year,\n",
    "    base_folder=\"data\",\n",
    "    output_folder=\"gamelogs\",\n",
    "    sleep_time=3,\n",
    "    max_retries=3,\n",
    "    verbose=True,\n",
    "):\n",
    "    season_folder = os.path.join(base_folder, str(year))\n",
    "\n",
    "    if not os.path.exists(season_folder):\n",
    "        print(f\"[ERROR] Folder not found: {season_folder}\")\n",
    "        return\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.sports-reference.com/\"\n",
    "    })\n",
    "\n",
    "    csv_files = [f for f in os.listdir(season_folder) if f.endswith(\".csv\")]\n",
    "    print(f\"Found {len(csv_files)} school CSVs for {year}\")\n",
    "\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    skip_count = 0\n",
    "    total_attempted = 0\n",
    "\n",
    "    for file_idx, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(season_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if \"player_sr_link\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            total_attempted += 1\n",
    "\n",
    "            player_url = row.get(\"player_sr_link\")\n",
    "            player_name = row.get(\"player_name\")\n",
    "            school = row.get(\"school\")\n",
    "            season = row.get(\"season\")\n",
    "\n",
    "            if not isinstance(player_url, str):\n",
    "                continue\n",
    "\n",
    "            safe_name = re.sub(r'[^a-zA-Z0-9_]+', '_', str(player_name))\n",
    "            safe_school = re.sub(r'[^a-zA-Z0-9_]+', '_', str(school))\n",
    "\n",
    "            team_folder = os.path.join(output_folder, str(year), safe_school)\n",
    "            os.makedirs(team_folder, exist_ok=True)\n",
    "\n",
    "            output_file = os.path.join(\n",
    "                team_folder,\n",
    "                f\"{safe_name}_{safe_school}_{year}.csv\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                skip_count += 1\n",
    "                if verbose:\n",
    "                    print(f\"[SKIP EXISTS] {output_file}\")\n",
    "                continue\n",
    "\n",
    "            player_url = player_url.replace(\".html\", \"\").rstrip(\"/\")\n",
    "            gamelog_url = f\"{player_url}/gamelog/{year}\"\n",
    "\n",
    "            print(f\"\\n[{file_idx+1}/{len(csv_files)} | player {idx+1}] {player_name}\")\n",
    "            print(f\"[URL] {gamelog_url}\")\n",
    "\n",
    "            success = False\n",
    "\n",
    "            for attempt in range(max_retries):\n",
    "                wait = 5 * (attempt + 1)\n",
    "\n",
    "                try:\n",
    "                    resp = session.get(gamelog_url, timeout=30)\n",
    "\n",
    "                    if resp.status_code == 429:\n",
    "                        print(\"[429 RATE LIMITED — sleeping 90s]\")\n",
    "                        time.sleep(90)\n",
    "                        continue\n",
    "\n",
    "                    if resp.status_code != 200:\n",
    "                        print(f\"[BAD STATUS {resp.status_code}] retrying\")\n",
    "                        time.sleep(wait)\n",
    "                        continue\n",
    "\n",
    "                    html = resp.text\n",
    "\n",
    "                    if \"Just a moment\" in html:\n",
    "                        print(\"[CLOUDFLARE BLOCK — sleeping 120s]\")\n",
    "                        time.sleep(120)\n",
    "                        continue\n",
    "\n",
    "                    sel = Selector(text=html)\n",
    "                    table = sel.css(\"table#player_game_log\")\n",
    "\n",
    "                    if not table:\n",
    "                        comments = sel.xpath(\"//comment()\").getall()\n",
    "                        for c in comments:\n",
    "                            if \"player_game_log\" in c:\n",
    "                                match = re.search(\n",
    "                                    r'(<table[^>]*id=\"player_game_log\"[^>]*>.*?</table>)',\n",
    "                                    c,\n",
    "                                    re.DOTALL\n",
    "                                )\n",
    "                                if match:\n",
    "                                    table = Selector(text=match.group(1))\n",
    "                                    break\n",
    "\n",
    "                    if not table:\n",
    "                        match = re.search(\n",
    "                            r'(<table[^>]*id=\"player_game_log\"[^>]*>.*?</table>)',\n",
    "                            html,\n",
    "                            re.DOTALL\n",
    "                        )\n",
    "                        if match:\n",
    "                            table = Selector(text=match.group(1))\n",
    "\n",
    "                    if not table:\n",
    "                        print(f\"[NO TABLE retry {attempt+1}]\")\n",
    "                        time.sleep(wait)\n",
    "                        continue\n",
    "\n",
    "                    rows = table.css(\"tbody tr\")\n",
    "                    player_games = []\n",
    "\n",
    "                    for r in rows:\n",
    "                        if \"thead\" in r.attrib.get(\"class\", \"\"):\n",
    "                            continue\n",
    "\n",
    "                        game = {\n",
    "                            cell.attrib[\"data-stat\"]: cell.css(\"::text\").get()\n",
    "                            for cell in r.css(\"th, td\")\n",
    "                            if \"data-stat\" in cell.attrib\n",
    "                        }\n",
    "\n",
    "                        game[\"player_name\"] = player_name\n",
    "                        game[\"player_sr_link\"] = player_url + \".html\"\n",
    "                        game[\"school\"] = school\n",
    "                        game[\"season\"] = season\n",
    "\n",
    "                        player_games.append(game)\n",
    "\n",
    "                    if player_games:\n",
    "                        pd.DataFrame(player_games).to_csv(output_file, index=False)\n",
    "                        print(f\"[SAVED] {output_file}\")\n",
    "                        success = True\n",
    "                        success_count += 1\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR attempt {attempt+1}] {e}\")\n",
    "                    time.sleep(wait)\n",
    "\n",
    "            if not success:\n",
    "                print(f\"[FAILED] {player_name}\")\n",
    "                fail_count += 1\n",
    "\n",
    "            # ---- progress line ----\n",
    "            print(\n",
    "                f\"Progress → Success: {success_count} | Failed: {fail_count} | \"\n",
    "                f\"Skipped: {skip_count} | Total Tried: {total_attempted}\"\n",
    "            )\n",
    "\n",
    "            time.sleep(sleep_time + random.uniform(1, 2))\n",
    "\n",
    "    print(\"\\n==========================\")\n",
    "    print(\"SCRAPE SUMMARY\")\n",
    "    print(\"==========================\")\n",
    "    print(f\"Success: {success_count}\")\n",
    "    print(f\"Failed: {fail_count}\")\n",
    "    print(f\"Skipped existing: {skip_count}\")\n",
    "    print(f\"Total attempted: {total_attempted}\")\n"
   ],
   "id": "2f373a742fecb568",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scrape_gamelogs_to_team_folders_v2(year=2020)",
   "id": "d89bb65168f84a7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scrape_gamelogs_to_team_folders_v2(year=2019, sleep_time=1)",
   "id": "a27d6fc381c2cabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T02:57:27.376412Z",
     "start_time": "2026-02-16T22:20:32.781109Z"
    }
   },
   "cell_type": "code",
   "source": "scrape_gamelogs_to_team_folders_v2(year=2018, sleep_time=1)",
   "id": "97c7ed3597859b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "SCRAPE SUMMARY\n",
      "==========================\n",
      "Success: 4362\n",
      "Failed: 12\n",
      "Skipped existing: 0\n",
      "Total attempted: 4374\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T10:19:46.133209Z",
     "start_time": "2026-02-17T06:12:42.957851Z"
    }
   },
   "cell_type": "code",
   "source": "scrape_gamelogs_to_team_folders_v2(year=2017, sleep_time=1)",
   "id": "cd157b4b1103faf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "SCRAPE SUMMARY\n",
      "==========================\n",
      "Success: 4446\n",
      "Failed: 2\n",
      "Skipped existing: 0\n",
      "Total attempted: 4448\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
